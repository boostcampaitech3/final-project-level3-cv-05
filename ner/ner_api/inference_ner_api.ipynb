{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T09:44:13.362955Z",
     "start_time": "2019-11-27T09:44:07.902764Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 05:09:49.095091: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-06-04 05:09:49.095133: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "/opt/conda/envs/ner_api/lib/python3.8/site-packages/konlpy/tag/_okt.py:16: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/opt/ml/ocr/ner/ner_api/app')\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from model.net import KobertCRF\n",
    "from data_utils.utils import Config\n",
    "from data_utils.vocab_tokenizer import Tokenizer\n",
    "from data_utils.vocab_tokenizer import Vocabulary\n",
    "from data_utils.pad_sequence import keras_pad_fn\n",
    "from pathlib import Path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T09:44:13.406972Z",
     "start_time": "2019-11-27T09:44:13.370406Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderFromNamedEntitySequence():\n",
    "    def __init__(self, tokenizer, index_to_ner):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.index_to_ner = index_to_ner\n",
    "\n",
    "    def __call__(self, list_of_input_ids, list_of_pred_ids):\n",
    "        input_token = self.tokenizer.decode_token_ids(list_of_input_ids)[0]\n",
    "        pred_ner_tag = [self.index_to_ner[pred_id] for pred_id in list_of_pred_ids[0]]\n",
    "\n",
    "        list_of_ner_word = []\n",
    "        entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "        for i, pred_ner_tag_str in enumerate(pred_ner_tag):\n",
    "            if \"B-\" in pred_ner_tag_str:\n",
    "                entity_tag = pred_ner_tag_str[-3:]\n",
    "\n",
    "                if prev_entity_tag != entity_tag and prev_entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\": entity_word.replace(\"▁\", \" \"), \"tag\": prev_entity_tag})\n",
    "\n",
    "                entity_word = input_token[i]\n",
    "                prev_entity_tag = entity_tag\n",
    "            elif \"I-\"+entity_tag in pred_ner_tag_str:\n",
    "                entity_word += input_token[i]\n",
    "            else:\n",
    "                if entity_word != \"\" and entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\":entity_word.replace(\"▁\", \" \"), \"tag\":entity_tag})\n",
    "                entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "\n",
    "        return list_of_ner_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namecard Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "\n",
    "model_dir = Path(ROOT_PATH / 'config')\n",
    "model_config = Config(json_path=model_dir / 'config.json')\n",
    "\n",
    "tok_path = str(ROOT_PATH / 'app/ptr_lm_model/tokenizer_78b3253a26.model')\n",
    "\n",
    "checkpoint_path = str(ROOT_PATH / 'checkpoints/best-epoch-12-step-1000-acc-0.961.bin')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_tokenizer(tok_path: str) -> Tuple[SentencepieceTokenizer, Vocabulary]:\n",
    "    ptr_tokenizer = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "    with open(model_dir / \"vocab.pkl\", 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    return ptr_tokenizer, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_to_index(model_dir: str) -> Tuple[Dict, Dict]:\n",
    "    with open(model_dir / \"ner_to_index.json\", 'rb') as f:\n",
    "        ner_to_index = json.load(f)\n",
    "        index_to_ner = {v: k for k, v in ner_to_index.items()}\n",
    "    return ner_to_index, index_to_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_config: Config, ner_to_index: Dict, vocab: Vocabulary, checkpoint_path: checkpoint_path = None):\n",
    "    model = KobertCRF(config=model_config, num_classes=len(ner_to_index), vocab=vocab)\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    if checkpoint_path:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "        convert_keys = {}\n",
    "        for k, v in checkpoint['model_state_dict'].items():\n",
    "            new_key_name = k.replace(\"module.\", '')\n",
    "            if new_key_name not in model_dict:\n",
    "                print(\"{} is not int model_dict\".format(new_key_name))\n",
    "                continue\n",
    "            convert_keys[new_key_name] = v\n",
    "\n",
    "        model.load_state_dict(convert_keys)\n",
    "    \n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ner_result(all_text_list: List) -> List:\n",
    "    ner_result = []\n",
    "\n",
    "    for input_text in all_text_list:\n",
    "        \n",
    "        list_of_input_ids = tokenizer.list_of_string_to_list_of_cls_sep_token_ids([input_text])\n",
    "        x_input = torch.tensor(list_of_input_ids).long().to(device)\n",
    "        list_of_pred_ids = model(x_input)\n",
    "\n",
    "        list_of_ner_word = decoder_from_res(list_of_input_ids=list_of_input_ids, list_of_pred_ids=list_of_pred_ids)\n",
    "        ner_result.append([word for word in list_of_ner_word if word['tag'] == 'PER' or word['tag'] == 'ORG'])\n",
    "\n",
    "    return ner_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptr_tokenizer, vocab = get_vocab_tokenizer(tok_path)\n",
    "tokenizer = Tokenizer(vocab=vocab, split_fn=ptr_tokenizer, pad_fn=keras_pad_fn, maxlen=model_config.maxlen)\n",
    "ner_to_index, index_to_ner = get_ner_to_index(model_dir)\n",
    "model = get_model(model_config, ner_to_index, vocab, checkpoint_path)\n",
    "\n",
    "model.eval()\n",
    "decoder_from_res = DecoderFromNamedEntitySequence(tokenizer=tokenizer, index_to_ner=index_to_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_list = ['김민수 삼성전자 tom 서울특별시 박민수 영등포구 123-45 010-1234-5687', '경기도 수원시 장안구 010-5456-5654 이지연 LG전자']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_result = get_ner_result(all_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'word': ' 김민수', 'tag': 'PER'},\n",
       "  {'word': ' 삼성전자', 'tag': 'ORG'},\n",
       "  {'word': ' 박민수', 'tag': 'PER'}],\n",
       " [{'word': ' 이지연', 'tag': 'PER'}, {'word': ' LG전자', 'tag': 'ORG'}]]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_list(ner_result: List) -> List:\n",
    "    result_list = []\n",
    "\n",
    "    for namecard in ner_result:\n",
    "        dict_per_namecard = {'PER': [], 'ORG': []}\n",
    "        for word_and_tag in namecard:\n",
    "            word, tag = word_and_tag.values()\n",
    "            if tag == 'PER':\n",
    "                dict_per_namecard['PER'].append(word)\n",
    "            elif tag == 'ORG':\n",
    "                dict_per_namecard['ORG'].append(word)\n",
    "        \n",
    "        result_list.append(dict_per_namecard)\n",
    "    \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = get_result_list(ner_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'PER': [' 김민수', ' 박민수'], 'ORG': [' 삼성전자']},\n",
       " {'PER': [' 이지연'], 'ORG': [' LG전자']}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "00c50005c56d1902dadfcb6ca5ad3f70aa4b52e057fc2871c205d46f2fad5dd0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ner_api')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
