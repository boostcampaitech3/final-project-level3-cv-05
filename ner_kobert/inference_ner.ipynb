{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T09:44:13.362955Z",
     "start_time": "2019-11-27T09:44:07.902764Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from model.net import KobertCRF\n",
    "from data_utils.utils import Config\n",
    "from data_utils.vocab_tokenizer import Tokenizer\n",
    "from data_utils.pad_sequence import keras_pad_fn\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T09:44:13.406972Z",
     "start_time": "2019-11-27T09:44:13.370406Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderFromNamedEntitySequence():\n",
    "    def __init__(self, tokenizer, index_to_ner):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.index_to_ner = index_to_ner\n",
    "\n",
    "    def __call__(self, list_of_input_ids, list_of_pred_ids):\n",
    "        input_token = self.tokenizer.decode_token_ids(list_of_input_ids)[0]\n",
    "        pred_ner_tag = [self.index_to_ner[pred_id] for pred_id in list_of_pred_ids[0]]\n",
    "\n",
    "        # ----------------------------- parsing list_of_ner_word ----------------------------- #\n",
    "        list_of_ner_word = []\n",
    "        entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "        for i, pred_ner_tag_str in enumerate(pred_ner_tag):\n",
    "            if \"B-\" in pred_ner_tag_str:\n",
    "                entity_tag = pred_ner_tag_str[-3:]\n",
    "\n",
    "                if prev_entity_tag != entity_tag and prev_entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\": entity_word.replace(\"▁\", \" \"), \"tag\": prev_entity_tag, \"prob\": None})\n",
    "\n",
    "                entity_word = input_token[i]\n",
    "                prev_entity_tag = entity_tag\n",
    "            elif \"I-\"+entity_tag in pred_ner_tag_str:\n",
    "                entity_word += input_token[i]\n",
    "            else:\n",
    "                if entity_word != \"\" and entity_tag != \"\":\n",
    "                    list_of_ner_word.append({\"word\":entity_word.replace(\"▁\", \" \"), \"tag\":entity_tag, \"prob\":None})\n",
    "                entity_word, entity_tag, prev_entity_tag = \"\", \"\", \"\"\n",
    "\n",
    "\n",
    "        # ----------------------------- parsing decoding_ner_sentence ----------------------------- #\n",
    "        decoding_ner_sentence = \"\"\n",
    "        is_prev_entity = False\n",
    "        prev_entity_tag = \"\"\n",
    "        is_there_B_before_I = False\n",
    "\n",
    "        for i, (token_str, pred_ner_tag_str) in enumerate(zip(input_token, pred_ner_tag)):\n",
    "            if i == 0 or i == len(pred_ner_tag)-1: # remove [CLS], [SEP]\n",
    "                continue\n",
    "            token_str = token_str.replace('▁', ' ')  # '▁' 토큰을 띄어쓰기로 교체\n",
    "\n",
    "            if 'B-' in pred_ner_tag_str:\n",
    "                if is_prev_entity is True:\n",
    "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>'\n",
    "\n",
    "                if token_str[0] == ' ':\n",
    "                    token_str = list(token_str)\n",
    "                    token_str[0] = ' <'\n",
    "                    token_str = ''.join(token_str)\n",
    "                    decoding_ner_sentence += token_str\n",
    "                else:\n",
    "                    decoding_ner_sentence += '<' + token_str\n",
    "                is_prev_entity = True\n",
    "                prev_entity_tag = pred_ner_tag_str[-3:] # 첫번째 예측을 기준으로 하겠음\n",
    "                is_there_B_before_I = True\n",
    "\n",
    "            elif 'I-' in pred_ner_tag_str:\n",
    "                decoding_ner_sentence += token_str\n",
    "\n",
    "                if is_there_B_before_I is True: # I가 나오기전에 B가 있어야하도록 체크\n",
    "                    is_prev_entity = True\n",
    "            else:\n",
    "                if is_prev_entity is True:\n",
    "                    decoding_ner_sentence += ':' + prev_entity_tag+ '>' + token_str\n",
    "                    is_prev_entity = False\n",
    "                    is_there_B_before_I = False\n",
    "                else:\n",
    "                    decoding_ner_sentence += token_str\n",
    "\n",
    "        return list_of_ner_word, decoding_ner_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namecard Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input json path\n",
    "json_path = '/opt/ml/ocr/info_val.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "anns = json_data['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = []\n",
    "\n",
    "for ann in anns:\n",
    "    bboxs_per_image = ann['ocr']['word']\n",
    "\n",
    "    words_in_image = ''\n",
    "    for bbox in bboxs_per_image:\n",
    "        words_in_image += bbox['text'] + ' '\n",
    "    words_in_image += '.'\n",
    "    all_words_list.append(words_in_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def namecard_test():\n",
    "    model_dir = Path('./experiments/base_model_with_crf')\n",
    "    model_config = Config(json_path=model_dir / 'config.json')\n",
    "\n",
    "    # load vocab & tokenizer\n",
    "    tok_path = \"./ptr_lm_model/tokenizer_78b3253a26.model\"\n",
    "    ptr_tokenizer = SentencepieceTokenizer(tok_path)\n",
    "\n",
    "    with open(model_dir / \"vocab.pkl\", 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    tokenizer = Tokenizer(vocab=vocab, split_fn=ptr_tokenizer, pad_fn=keras_pad_fn, maxlen=model_config.maxlen)\n",
    "\n",
    "    # load ner_to_index.json\n",
    "    with open(model_dir / \"ner_to_index.json\", 'rb') as f:\n",
    "        ner_to_index = json.load(f)\n",
    "        index_to_ner = {v: k for k, v in ner_to_index.items()}\n",
    "\n",
    "    # model\n",
    "    model = KobertCRF(config=model_config, num_classes=len(ner_to_index), vocab=vocab)\n",
    "\n",
    "    # load\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # checkpoint_path 수정 - Fix me\n",
    "    checkpoint_path = '/opt/ml/ocr/ner/ner_kobert/experiments/base_model_with_crf_val/best-epoch-12-step-1000-acc-0.961.bin'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "    convert_keys = {}\n",
    "    for k, v in checkpoint['model_state_dict'].items():\n",
    "        new_key_name = k.replace(\"module.\", '')\n",
    "        if new_key_name not in model_dict:\n",
    "            print(\"{} is not int model_dict\".format(new_key_name))\n",
    "            continue\n",
    "        convert_keys[new_key_name] = v\n",
    "\n",
    "    model.load_state_dict(convert_keys)\n",
    "    model.eval()\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "    decoder_from_res = DecoderFromNamedEntitySequence(tokenizer=tokenizer, index_to_ner=index_to_ner)\n",
    "\n",
    "    tag_result = {\n",
    "    'PER': [],\n",
    "    'LOC': [],\n",
    "    'ORG': [],\n",
    "    'POH': [],\n",
    "    'DAT': [],\n",
    "    'TIM': [],\n",
    "    'DUR': [],\n",
    "    'MNY': [],\n",
    "    'PNT': [],\n",
    "    'NOH': []\n",
    "    }\n",
    "\n",
    "    for input_text in all_words_list:\n",
    "        \n",
    "        list_of_input_ids = tokenizer.list_of_string_to_list_of_cls_sep_token_ids([input_text])\n",
    "        x_input = torch.tensor(list_of_input_ids).long().to(device)\n",
    "        list_of_pred_ids = model(x_input)\n",
    "\n",
    "        list_of_ner_word, decoding_ner_sentence = decoder_from_res(list_of_input_ids=list_of_input_ids, list_of_pred_ids=list_of_pred_ids)\n",
    "        print(\"output>\", decoding_ner_sentence)\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"\")\n",
    "\n",
    "        for tags in decoding_ner_sentence.replace('>', '<').split('<'):\n",
    "            if ':' in tags:\n",
    "                word, tag = tags.split(':')[-2:]\n",
    "                if tag in tag_result.keys():\n",
    "                    tag_result[tag].append(word)\n",
    "\n",
    "    return tag_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### NER Tagset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- 8개의 태그  \n",
    "  - PER: 사람이름\n",
    "  - LOC: 지명\n",
    "  - ORG: 기관명\n",
    "  - POH: 기타\n",
    "  - DAT: 날짜\n",
    "  - TIM: 시간\n",
    "  - DUR: 기간\n",
    "  - MNY: 통화\n",
    "  - PNT: 비율\n",
    "  - NOH: 기타 수량표현\n",
    "- 개체의 범주\n",
    "  - 개체이름: 사람이름(PER), 지명(LOC), 기관명(ORG), 기타(POH)\n",
    "  - 시간표현: 날짜(DAT), 시간(TIM), 기간 (DUR)\n",
    "  - 수량표현: 통화(MNY), 비율(PNT), 기타 수량표현(NOH)\n",
    "result = namecard_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = namecard_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num of Tags\")\n",
    "for tag in result.keys():\n",
    "    print(f\"{tag} : {len(result[tag])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example')\n",
    "for tag in result.keys():\n",
    "    print(f\"{tag} : {result[tag][:50]}\")\n",
    "    # print(f\"{tag} : {result[tag]}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b82ab4dd17799953bba2d6e733a24c6fbe5a0c8eeacef3d76881ddb1dc9793"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ocr_new')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
